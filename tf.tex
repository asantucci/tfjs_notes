\documentclass[12pt]{article}  

\usepackage[boxruled,lined]{algorithm2e}
%% \usepackage{booktabs}
\usepackage{amsmath} 
\usepackage{amsthm} 
\usepackage{amsfonts} 
\usepackage{enumitem}
\usepackage[T1]{fontenc}
\usepackage{xparse} 
\usepackage{bm}
\usepackage{bbm} 
\usepackage{color,soul} 
\usepackage{framed}
\usepackage[margin=0.5in]{geometry}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{multicol}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz}
\usepackage[normalem]{ulem}
\usepackage{pgfplots}  
\usepackage{pifont}
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{fit}
\usetikzlibrary{shapes}
\usetikzlibrary{patterns}
\usetikzlibrary{decorations.pathreplacing}
\newcommand\STAR{{\tikz{\node[draw,star,star point height=.7em,minimum size=1em,scale=0.35]{};} }}
\newcommand{\Plus}{\mathord{\begin{tikzpicture}[baseline=0ex, line width=1, scale=0.13]
\draw (1,0) -- (1,2); \draw (0,1) -- (2,1); \end{tikzpicture}}}

\lstdefinelanguage{JavaScript}{
  keywords={async, await, break, case, catch, const, continue, debugger, default, delete, do, else, finally, for, function, if, in, instanceof, new, return, switch, this, throw, try, typeof, var, void, while, with},
  morecomment=[l]{//},
  morecomment=[s]{/*}{*/},
  morestring=[b]',
  morestring=[b]",
  sensitive=true
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\D}{\mathrm{d}}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}

\begin{document}
\renewcommand{\d}[1]{\ensuremath{\operatorname{d}\!{#1}}}

\tableofcontents
\newpage
\section{Browser Based Models with TensorFlow.js}\vspace{.1pt} \hrule height 2pt \smallskip \renewcommand{\arraystretch}{1}
\subsection{Training and Inference using Tensorflow.js in JavaScript} We typically think of training a neural network using GPU's or a large data center, but modern web browsers have come a long way: they contain fully fleshed out runtime environments. One of the exciting aspects of Tensorflow.js is that it allows us to train neural networks and perform inference directly from a browser. We'll see how, for example, a user can upload a picture or grab a snapshot from a webcam and then train a neural network and/or perform inference right in the browser, without ever needing to send that image up to the cloud to be processed by a server. This saves time as we cut down on communication costs,
allows us to run our models offline, and preserves user privacy.

\subsubsection{Getting Your System Ready} We're going to learn how to run all the examples and exercises for this course locally on a single machine. We'll use \href{https://www.google.com/chrome/}{Chrome} for a browser, \href{http://brackets.io/}{Brackets} for an HTML editor, and the \href{https://chrome.google.com/webstore/detail/web-server-for-chrome/ofhbbkphhbklhfoeikjpcbhemlocgigb?hl=en}{Web Server for Chrome App} as our web server. We will also use \href{https://github.com/lmoroney/dlaicourse}{github.com/lmoroney/dlaicourse} as a repository to store homework assignments.

\subsubsection{API Stack}
\begin{itemize}
\item At the highest level, we have the \emph{Keras Model} layers API which   we've learned how to use in the deep learning specialization.
\item Beneath this sits the \emph{Core API}, which is backed by a TensorFlow saved model. Using this Core API, we can interact with either a browser or node.js, and this is critically what allows us to use things like layers from keras.
\begin{itemize}
\item A browser sits ontop of \href{https://en.wikipedia.org/wiki/WebGL}{webgl},   a JavaScript API for rendering graphics; we implicitly have access to a GPU here!
\item \emph{Node.js} can rely on a TensorFlow CPU, GPU, or TPU.
\end{itemize}
\end{itemize}

\subsubsection{Building a First Model} Let's start by creating the simplest possible web-page.
\begin{verbatim}
<html>
<head></head>
<body>
  <hl>First HTML Page</hl>
</body>
</html>
\end{verbatim}

We'll next need to add a \emph{script tag} below the head and above the body to load the TensorFlow.js file.

\begin{verbatim}
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
\end{verbatim}

We're going to build a model that learns the relationship between two numbers $x,y$ where their ground truth relationship is $y = 2x - 1$. We will do this in a \emph{separate script block}, that needs to be placed above the \texttt{body} tag in your HTML page.

\begin{verbatim}
<script lang="js">
    const model = tf.sequential();
    model.add(tf.layers.dense({units: 1, inputShape: [1]}));
    model.compile({loss:'meanSquaredError',
                   optimizer:'sgd'});
    model.summary();
</script>
\end{verbatim}

The first line defines a \emph{sequential} model. The second line adds a single hidden layer, itself containing a single hidden neuron. We then compile the model using mean-squared-error as our loss function, which will work well to model a linear relationship $y=2x-1$, and we choose stochastic gradient descent as our method of optimization. Note that in the model summary, we'll be told there are two parameters in the model, since we are learning both a weight \emph{and} a bias term. Before closing the script tag, let's insert some data that will be used to train the neural network.

\begin{verbatim}
const xs = tf.tensor2d([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], [6, 1]);
const ys = tf.tensor2d([-3.0, -1.0, 2.0, 3.0, 5.0, 7.0], [6, 1]);
\end{verbatim}
Notice that we're defining this as a \texttt{tensor2d}, since we don't have something like \texttt{numpy} from Python. As its name suggests, we must specify the extents of the two dimensions via the second argument.

\subsubsection{Training the Model} Training should by \emph{asynchronous}, because it takes an indeterminate amount of time to complete. Our next piece of code will call an asynchronous function called \texttt{doTraining}, which, when it completes execution will do something. Because training can take an indeterminate amount of time, we don't want to block the browser while this is taking place, so instead we specify it as an asynchronous function that calls us back when it's done.
\begin{lstlisting}[language=JavaScript]
doTraining(model).then(() => {
    alert(model.predict(tf.tensor2d([10], [1,1])));
});
\end{lstlisting}
We call the function by passing it a model we just created above; when it calls back, the model is trained and at that point we can call \texttt{model.predict()}. In this example, we're predicted $\hat y$ for an input of $x = 10$. Note that we still must use a \texttt{tensor2d}, which in this case is just a scalar i.e. a tensor of dimension $1 \times 1$. To actually define an asynchronous training function, we can do so as follows:

\begin{lstlisting}[language=JavaScript]
  async function doTraining(model){
      const history =
            await model.fit(xs, ys,
                  { epochs: 500,
                    callbacks:{
                        onEpochEnd: async(epoch, logs) =>{
                            console.log("Epoch:" + epoch + " Loss:" + logs.loss);
                        }
                    }
                  });
  }
\end{lstlisting}

This code should be placed at the top of the script block that we've been creating. Because \texttt{doTraining} is asynchronous, we use keyword \texttt{await} to wait for the result. Note that after feeding the \texttt{xs} and \texttt{ys} as input arguments, the rest of the input is a JSON list, with each list-item denoted by a name followed by a colon, followed by a value. For callbacks, we can specify it on the fly in the list, where the callback itself is defined as a \texttt{list} and \texttt{onEpochEnd} is a function. To be clear, we're adding a function as a list-item. In our example, upon each epoch ending, we take the epoch number and \texttt{logs} as parameters so we can print out the information to console.

Because \texttt{doTraining} is asynchronous, when we call it we'll use a \texttt{then} clause to specify what should happen upon completion of execution.

\subsection{Training Models with CSV Files}
\paragraph{The \href{https://archive.ics.uci.edu/ml/datasets/iris}{Iris         Dataset}} We've covered a simple example where we stored the values of our data in memory. A more commmon scenario is when data comes in from an outside source such as a database connection or an imported dataset. One of the most common ways of getting data into an ML model, and JavaScript ones are no exception, is reading data from CSV files. TensorFlow.js provides facilities for this. We'll work with an \emph{Iris} dataset, which contains 150 samples taken from three types of Iris flower with 50 from each type. There are four features and a label for each observation.

\subsubsection{Reading the Data} How can we do this using JavaScript and TensorFlow.js. Before we code, we can examine our CSV file. Note the first line contains column headers. The features encode $\{\textrm{sepal}, \textrm{petal}\} \times \{\textrm{length}, \textrm{width}\}$. We'll start by placing an asynchronous function into a JavaScript block; it must be asynchronous because we'll be waiting for some values, e.g. while training.

\begin{lstlisting}[language=JavaScript]
  asynch function run() { 
  }
\end{lstlisting}

From here on out in this running example, the remainder of the code will be placed within the definition of this \texttt{run()} function. When we are done defining it, we will call it in our script.
The first things we'll do is to load the data from a CSV, for which we'll use the \texttt{tf.data.csv} class to handle parsing the data.
\begin{lstlisting}[language=JavaScript]
  const csvUrl = 'iris.csv';
  const trainingData = tf.data.csv(csvUrl, {
    columnConfigs: {
      species: {
        isLabel: true
      }
    }
  });
\end{lstlisting}

There are some important details to note. The CSV resides at a URL: we don't have the server or protocol details, which means its going to try and load it from the same directory as the web-page which is hosting the application. But, it's critical to note that we're not loading from the file system directly; it's going through the HTTP stack to get the file, so you'll need to run this code on a web server.\footnote{What's nice about the Brackets IDE is that it has a built-in web server.} In defining our training data, we make a call to \texttt{tf.data.csv} passing the URL; since tensorflow doesn't know anything about our features or labels, and so we instruct it which is our label via list syntax. In particular, we're specifying that the \texttt{Species} column is a label. After having loaded our data in this way, tensorflow.js will recognize that the Species column is a label and therefore part of the \texttt{y} set, and the remaining features are part of the \texttt{x}'s set.

The data are returned from \texttt{tf.data.csv} as dictionaries, and for training we'll want to convert them into arrays. We'll also one-hot encode our string labels. To create a one-hot encoding, we can do so as follows:

\begin{lstlisting}[language=JavaScript]
  const convertedData =
    trainingData.map(( {xs, ys} => {
      const labels = [
        ys.species == "setosa" ? 1 : 0,
        ys.species == "virginica" ? 1 : 0,
        ys.species == "versicolor" ? 1 : 0 ]
        return{ xs: Object.values(xs), ys: Object.values(labels) };
      }).batch(10);
\end{lstlisting}
In calling the \texttt{map} method on our \texttt{trainingData}, we will iterate over each training example. The values that werent flagged as labels are in the \texttt{xs} data-structure. If we call \texttt{Object.values(xs)}, we get back an array-of-arrays containing their values. Each row in the data-set had four features which yields a $4 \times 1$ array. These are then loaded into an array of length the size of the dataset (in this case 150 examples). Notice that we're also calling \texttt{Object.values(labels)}, which returns an array-of-arrays back as well (in this case each label is a $3 \times 1$ array and we'll have 150 of them). Ultimately, our function returns a set of features that we'll train on alongside one-hot encoded labels.

\subsubsection{Designing the Neural Network} We'll choose to create a neural network with the following architecture: four features map to a single hidden layer with five neurons which then map to an output layer with three nodes that we'll use for classification. This is how what that looks like in code:
\begin{lstlisting}[language=JavaScript]
  const model = tf.sequential();

  model.add(tf.layers.dens({
    input.shape: [numOfFeatures],
    activation: "sigmoid", units: 5}))

  model.add(tf.layers.dense({activation: "softmax", units: 3}));

  model.compile({
    loss: "categoricalCrossentropy",
    optimizer: tf.train.adam(0.06)})

  await model.fitDataset(
    convertedData,
    {
      epochs: 100,
      callbacks: {
        onEpochEnd: async(epoch, logs) => {
          console.log("E: " + epoch + " Loss: " + logs.loss);
        }
      }
    });
\end{lstlisting}

Notice that we're passing our \texttt{convertedData} as argument to our training method. Here, we're using a slightly different function: \texttt{model.fitDataset}. By creating importing our CSV file as a dataset, we've already done a lot of the pre-processing necessary to plug-and-play with this function. We pass the data in as first parameter, then pass a list of JSON style name values with things such as number of epochs or callback behaviors defined. If we want to use the model to do inference and get a prediction, we must first create an input tensor with feature values and pass it to the predict method of the model. Also, it's critical that we use \texttt{await} so that we don't proceed with execution and try to make a prediction (see below) while our model is still training.

\begin{lstlisting}[language=JavaScript]
  const testVal = tf.tensor2d([5.8, 2.7, 5.1, 1.9], [1, 4]);
  const prediction = model.predict(testVal);
  alert(prediction);
\end{lstlisting}

If we hadn't used \texttt{await} above when training our model, our call to predict would yield funky results as we'd be performing inference on a model that isn't (fully) trained. 
Here, the \texttt{testVal} object holds the sepal and petal length and width; when we pass it to the predict method, we get a tensor back with a prediction in it. Here's the script in its entirety.

\lstinputlisting[basicstyle=\tiny]{html_examples/iris-classifier.html}

\paragraph{Instructions for Running the Iris-Classifier Application} First, we must open our web-server application; once launched, we must select which folder we'd like to run out of, which in this case is \texttt{dlaicourse.TensorFlow Deployment/Course 1 - TensorFlow-JS/Week 1/Examples/}. Once the folder is selected, click on the Web-Server URL which then opens a new tab in your Chrome browser. From here, we can click on the \texttt{html} file we want to run, in this case the \texttt{iris-classifier.html} file.

\subsection{Creating Convolutional Neural Networks in JavaScript} We've seen how we can get a simple neural network running in a browser, and this next section will teach more advanced concepts. Specifically, we'll build a convolutional neural network in a web-browser that can process images. In particular, we're going to work with MNIST dataset, which consists of tens-of-thousands of images; opening up 10k HTTP connections would be problematic, and instead we'll see how we can combine all images into a single spreadsheet file which can then be donwloaded, sliced, and trained on.

\subsubsection{Creating a Convolutional Net with JavaScript} Although we're familiar with coding CNN's using Python, there are a few minor syntactical changes to be aware of when coding in JavaScript. The largest discrepancies are outside of the model definition, with e.g. code that fetches the images or processes an image for classification. In this section, we will
\begin{itemize}
\item train a convolutional neural network for image classification in the   browser, and
\item write a browser app that takes these images and passes them to the classifier.
\end{itemize}
Let's look at the code for creating a conv-net in JavaScript.
\begin{lstlisting}[language=JavaScript]
  model = tf.sequential();
  model.add(tf.layers.conf2d({inputShape: [28, 28, 1],
            kernelSize: 3, filters: 8, activation: 'relu'}));
  model.add(tf.layers.maxPooling2d({poolSize: [2, 2]}));
  model.add(tf.layers.conv2d({filters: 16,
            kernelSize: 3, activation: 'relu'}));
  model.add(tf.layers.maxPooling2d({poolSize: [2, 2]}));
  model.add(tf.layers.flatten());
  model.add(tf.layers.dense({units: 128, activation: 'relu'}));
  model.add(tf.layers.dense({units: 10, activation: 'softmax'}));
\end{lstlisting}
By declaring our model as a \texttt{tf.sequential()}, we are specifying that the model will be defined via a \emph{sequence} of layers. Our first layer is a 2-d convolution, which takes in our $28 \times 28$ MNIST dataset in monochrome format. The parameter \texttt{kernelSize: 3} indicates we want $3 \times 3$ filters. By using a Rectified-Linear Unit, we're filtering out values less than zero. The next step specifies a $2 \times 2$ max-pooling step. The call to \texttt{tf.layers.flatten()} takes our 2-D image array and flattens it into a 1-D vector, from which we will attempt to learn a mapping to the 10 different possible output digits, i.e. the labels. To compile the model, we must specify a loss function and an optimizer, as well as any metrics we'd like to track; note that we pass parameters via a JavaScript dictionary (hence the braces).
\begin{lstlisting}[language=JavaScript]
  model.compile(
     {  optimizer: tf.train.adam(),
        loss: 'categoricalCrossEntropy',
        metrics: ['accuracy']
     };
\end{lstlisting}
Training the model is done with the \texttt{fit()} method.
\begin{lstlisting}[language=JavaScript]
  model.fit(trainXs, trainYs, {
    batchSize: BATCH_SIZE,
    validationData: [testXs, testYs],
    epochs: 20,
    shuffle: true,
    callbacks: fitCallbacks
});
\end{lstlisting}
We'll train our data in batches, which aside from theoretical benefits also prevents locking up the browser itself. Notice as well that we're allowing validation data to be passed so that we may report validation accuracy as we train. Shuffling the data helps prevent over-fitting if our data is grouped
by label. We can of course also specify custom callbacks; we'll look at how to use \texttt{tf-vis} to render the outputs of our callbacks in the next section.

\subsubsection{\href{https://github.com/tensorflow/tfjs/tree/master/tfjs-vis}{Visualizing the Training Process}}
There are some extra tools we can use to make the visualization of training a lot more friendly. The first step is to include the library \texttt{tfjs-vis} in your code with a script.
\begin{lstlisting}[language=JavaScript]
  <script src="https://cdn.jsdelivr.net/npm@tensorflow/tfjs-vis"></script>
\end{lstlisting}
We can get access to the code and documentation at \url{https://github.com/tensorflow/tfjs-vis}.
Previously, we had used a term \texttt{fitCallbacks} without talking about how its defined.
\begin{lstlisting}[language=JavaScript]
  const fitCallbacks = tfvis.show.fitCallbacks(container, metrics);
\end{lstlisting}
The \texttt{container} is responsible for rendering the feedback, and the \texttt{metric}s are what we are interested in tracking.
\begin{lstlisting}[language=JavaScript]
  const metrics = ['loss', 'val_loss', 'acc', 'val_acc'];
  const container = { name: 'Model Training', styles: { height: '1000px' } };
  const fitCallbacks = tfvis.show.fitCallbacks(container, metrics);
\end{lstlisting}
When we create the container, we just set a name and any required styles, and the visualization library will create the DOM elements to render the details. While training, the callback will create a container in which it will draw the feedback depending on the metrics we requested. Let's now segway to discuss how we handle training in browsers with lots of data.

\subsubsection{Sprite Sheets}
The last few videos shows how to train a model and visualize training progress. But in any training, we need data. MNIST contains 10's of thousands of images. When loading into Python, there's no issue. But in JavaScript we'd be making an HTTP request for each image, which at the very least is bad practice. One solution is to take all the images and stitch them into a sprite-sheet, which is a single spreadsheet which contains all images in the dataset.
The MNIST sprite-sheet is stored here: \url{https://storage.googleapis.com/learnjs-data/model-builder/mnist_images.png}.
\paragraph{Using the Sprite-Sheet} If we visit the above url, we'll see a single image that's 65,000 pixels tall and 784 pixels wide. Each $28 \times 28$ image is encoded into a row ($28^2 = 784$). The labels can be found at
\url{https://storage.googleapis.com/learnjs-data/model-builder/mnist_labels_uint8}. The file is 650k bytes in size, i.e. 10 bytes per image. To understand this binary file, we'll need a hex-viewer, but if we open it in this way we can see that each row corresponds to a one-hot encoding of the label (a digit 0-9); in each row of the file there are nine zero bytes and one byte that is flipped on. Although this is not necessarily an efficient file format, it's easy to one-hot encode in memory because it's serialized as a one-hot encoding already. We have a \texttt{data.JS} file which is intended to handle this.

\begin{lstlisting}[language=JavaScript]
  export class MnistData {
    ...
    async load() {
      // Download the sprite and slice it.
      // Download the labels and decode them.
    }
    nextTrainBatch() {
      // Get the next training batch.
    }
    nextTestBatch() {
      // Get the next test batch.
    }
\end{lstlisting}

The job of the \texttt{load} method is to download the spritesheet and labels, and decode them along with a helper function called \texttt{nextBatch} which is used to batch into specified train and test batch sizes. The other methods are responsible for getting batches of training data, i.e. slices off the image according to the desired size. Note that the \texttt{nextTrainBatch} keeps each image as a $1 \times 784$ vector of pixels and the calling function can resize them to $28 \times 28$. It also returns the appropriate labeled data. In order to initialize the data class and load the sprite, getting it ready for batching, all we need is the following code.
\begin{lstlisting}[language=JavaScript]
  const data = new MnistData();
  await data.load();
\end{lstlisting}
Once you have a loaded instance of the data, we can now get the batches and resize them to be $28 \times 28$ like this.
\begin{lstlisting}[language=JavaScript]
  const [trainXs, trainYs] = tf.tidy(() => {
    const d = data.nextTrainBatch(TRAIN_DATA_SIZE);
    return [
      d.xs.reshape([TRAIN_DATA_SIZE, 28, 28, 1]);
      d.labels
    ];
  });
\end{lstlisting}
We intend to create an array containing the set of $X$s and $Y$s for training. Our function does this by getting the next batch from the training data source. By default, with MNIST the training data size is 5,500, and so we're basically fetching 5,500 lines of 784 bytes. We then reshape the data into a four-dimensional tensor with 5,500 in the first dimension, then $28 \times 28$ representing the image, then $1$ representing the monochromatic color-depth. That's the first element of the array, the second is the labels which are already one-hot encoded.

\paragraph{Tidy Clause} The \texttt{tf.tidy()} let's us be good citizens of the web. The idea behind \texttt{tf.tidy()} is that once it's done using the allocated memory for the $5,500 \times 28 \times 28$ tensor, the runtime environment cleans up afterward. I.e. \texttt{d} gets cleaned up after we're done and it saves us a lot of memory.
\end{document}

